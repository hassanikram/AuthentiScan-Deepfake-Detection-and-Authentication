{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet-pytorch\n",
      "  Using cached efficientnet_pytorch-0.7.1-py3-none-any.whl\n",
      "Requirement already satisfied: albumentations in c:\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: opencv-python in c:\\anaconda3\\lib\\site-packages (4.7.0)\n",
      "Requirement already satisfied: pillow in c:\\anaconda3\\lib\\site-packages (10.3.0)\n",
      "Collecting captum\n",
      "  Obtaining dependency information for captum from https://files.pythonhosted.org/packages/e1/76/b21bfd2c35cab2e9a4b68b1977f7488c246c8cffa31e3361ee7610e8b5af/captum-0.7.0-py3-none-any.whl.metadata\n",
      "  Using cached captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting timm\n",
      "  Obtaining dependency information for timm from https://files.pythonhosted.org/packages/19/0d/57fe21d3bcba4832ed59bc3bf0f544e8f0011f8ccd6fd85bc8e2a5d42c94/timm-1.0.3-py3-none-any.whl.metadata\n",
      "  Using cached timm-1.0.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in c:\\anaconda3\\lib\\site-packages (from efficientnet-pytorch) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\anaconda3\\lib\\site-packages (from albumentations) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from albumentations) (1.10.1)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in c:\\anaconda3\\lib\\site-packages (from albumentations) (0.20.0)\n",
      "Requirement already satisfied: PyYAML in c:\\anaconda3\\lib\\site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: qudida>=0.0.4 in c:\\anaconda3\\lib\\site-packages (from albumentations) (0.0.4)\n",
      "Collecting opencv-python-headless>=4.1.1 (from albumentations)\n",
      "  Obtaining dependency information for opencv-python-headless>=4.1.1 from https://files.pythonhosted.org/packages/03/a1/ff49de25dc4e4a16f6b9f57d7ed302acabb450e92054a5be5ecc93e66fca/opencv_python_headless-4.10.0.82-cp37-abi3-win_amd64.whl.metadata\n",
      "  Using cached opencv_python_headless-4.10.0.82-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda3\\lib\\site-packages (from captum) (3.7.2)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\lib\\site-packages (from captum) (4.66.4)\n",
      "Requirement already satisfied: torchvision in c:\\anaconda3\\lib\\site-packages (from timm) (0.18.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\anaconda3\\lib\\site-packages (from timm) (0.15.1)\n",
      "Requirement already satisfied: safetensors in c:\\anaconda3\\lib\\site-packages (from timm) (0.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\anaconda3\\lib\\site-packages (from qudida>=0.0.4->albumentations) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\anaconda3\\lib\\site-packages (from qudida>=0.0.4->albumentations) (4.11.0)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (3.2.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2023.4.12)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (1.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (0.3)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda3\\lib\\site-packages (from torch->efficientnet-pytorch) (2023.4.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch->efficientnet-pytorch)\n",
      "  Obtaining dependency information for mkl<=2021.4.0,>=2021.1.1 from https://files.pythonhosted.org/packages/fe/1c/5f6dbf18e8b73e0a5472466f0ea8d48ce9efae39bd2ff38cebf8dce61259/mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2.32.2)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from tqdm->captum) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->captum) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\anaconda3\\lib\\site-packages (from matplotlib->captum) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\anaconda3\\lib\\site-packages (from matplotlib->captum) (2.9.0.post0)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch->efficientnet-pytorch)\n",
      "  Obtaining dependency information for intel-openmp==2021.* from https://files.pythonhosted.org/packages/6f/21/b590c0cc3888b24f2ac9898c41d852d7454a1695fbad34bee85dba6dc408/intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch->efficientnet-pytorch)\n",
      "  Obtaining dependency information for tbb==2021.* from https://files.pythonhosted.org/packages/7b/2d/1e1c70fae8ace27e6200fb71c2372a9aeac2baba474b1609d7d466e969b4/tbb-2021.12.0-py3-none-win_amd64.whl.metadata\n",
      "  Using cached tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda3\\lib\\site-packages (from jinja2->torch->efficientnet-pytorch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\anaconda3\\lib\\site-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\n",
      "Using cached captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
      "Using cached opencv_python_headless-4.10.0.82-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Using cached tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "Installing collected packages: tbb, intel-openmp, opencv-python-headless, mkl, efficientnet-pytorch, captum, timm\n",
      "Successfully installed captum-0.7.0 efficientnet-pytorch-0.7.1 intel-openmp-2021.4.0 mkl-2021.4.0 opencv-python-headless-4.10.0.82 tbb-2021.12.0 timm-1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet-pytorch albumentations opencv-python pillow captum timm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset & File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from torch import nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "#Kaggle Dataset path\n",
    "dataset='Celeb-Df-v2'\n",
    "\n",
    "#fake and real videos path\n",
    "real_videos_dir = os.path.join(dataset, 'Celeb-real')\n",
    "youtube_real_dir = os.path.join(dataset,'YouTube-real')\n",
    "fake_videos_dir = os.path.join(dataset, 'Celeb-synthesis')\n",
    "\n",
    "#test videos path\n",
    "test_dir= os.path.join(dataset, 'List_of_testing_videos.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrating frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create file paths for storing frames\n",
    "os.makedirs('frames/real',exist_ok=True)\n",
    "os.makedirs('frames/fake',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting faces function\n",
    "def extract_frames(video_path,output_file, interval=30):\n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % interval == 0:\n",
    "            frame_path = os.path.join(output_file, f\"{os.path.basename(video_path).split('.')[0]}_frame_{count}.jpg\")\n",
    "            cv2.imwrite(frame_path,frame)\n",
    "        count+=1\n",
    "    cap.release()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now extracting frames from real videos\n",
    "for video in os.listdir(real_videos_dir):\n",
    "    extract_frames(os.path.join(real_videos_dir,video), 'frames/real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now extracting frames from Youtube real videos\n",
    "for video in os.listdir(youtube_real_dir):\n",
    "    extract_frames(os.path.join(youtube_real_dir,video), 'frames/real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting frames from fake videos\n",
    "for video in os.listdir(fake_videos_dir):\n",
    "    extract_frames(os.path.join(fake_videos_dir,video), 'frames/fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splittig train test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#getting list of all extracted frame paths\n",
    "\n",
    "real_frames_path = [os.path.join('frames/real',file) for file in os.listdir('frames/real')]\n",
    "fake_frames_path = [os.path.join('frames/fake',file) for file in os.listdir('frames/fake')]\n",
    "\n",
    "#Creating labels: Real -> 0, Fake -> 1\n",
    "\n",
    "real_labels = [0] * len(real_frames_path)\n",
    "fake_labels = [1] * len(fake_frames_path)\n",
    "\n",
    "#Merge Real and Fake data\n",
    "\n",
    "all_frames = real_frames_path + fake_frames_path\n",
    "all_labels = real_labels + fake_labels\n",
    "\n",
    "#shuffle the merged data.\n",
    "\n",
    "combined_data = list(zip(all_frames,all_labels))\n",
    "random.shuffle(combined_data)\n",
    "all_frames,all_labels = zip(*combined_data)\n",
    "\n",
    "#split train and test data with 75%-25%\n",
    "\n",
    "split_index = int(0.75 * len(all_frames))\n",
    "xtrain,ytrain = all_frames[:split_index], all_labels[:split_index]\n",
    "xtest, ytest = all_frames[split_index:], all_labels[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85877\n",
      "12394\n",
      "73483\n",
      "64407 64407\n",
      "21470 21470\n"
     ]
    }
   ],
   "source": [
    "print(len(all_frames))\n",
    "print(len(real_frames_path))\n",
    "print(len(fake_frames_path))\n",
    "print(len(xtrain),len(ytrain))\n",
    "print(len(xtest),len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Create train and test directories\n",
    "os.makedirs('data/train/real')\n",
    "os.makedirs('data/train/fake')\n",
    "os.makedirs('data/test/real')\n",
    "os.makedirs('data/test/fake')\n",
    "\n",
    "# Function to copy files to the appropriate directory\n",
    "def copy_files(file_paths, labels, base_dir):\n",
    "    for file_path, label in zip(file_paths, labels):\n",
    "        if label == 0:\n",
    "            dest_dir = os.path.join(base_dir, 'real')\n",
    "        else:\n",
    "            dest_dir = os.path.join(base_dir, 'fake')\n",
    "        shutil.copy(file_path, dest_dir)\n",
    "\n",
    "# Copy training data\n",
    "copy_files(xtrain, ytrain, 'data/train')\n",
    "\n",
    "# Copy testing data\n",
    "copy_files(xtest, ytest, 'data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80542 21685\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Custom Dataset Class\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.dataset = ImageFolder(root=root)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.dataset.imgs[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = np.array(image)\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, label\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = FrameDataset(root='data/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=48, shuffle=True, num_workers= 0)\n",
    "test_dataset = FrameDataset(root= \"data/test\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=48, shuffle=True, num_workers= 0)\n",
    "\n",
    "\n",
    "print(len(train_dataset),len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n",
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "#New edited code with layers\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "class EfficientNetB4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetB4, self).__init__()\n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(self.model._fc.in_features, 512)\n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.model._fc = nn.Identity()  # Remove the original fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class EfficientNetAutoAttB4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetAutoAttB4, self).__init__()\n",
    "        self.base_model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=self.base_model._fc.in_features, num_heads=8, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(self.base_model._fc.in_features, 512)\n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.base_model._fc = nn.Identity()  # Remove the original fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.extract_features(x)  # Shape: [batch_size, channels, height, width]\n",
    "        x = x.permute(0, 2, 3, 1)  # Shape: [batch_size, height, width, channels]\n",
    "        x = x.reshape(x.size(0), -1, x.size(-1))  # Shape: [batch_size, seq_length, embed_dim]\n",
    "        attn_output, _ = self.attention_layer(x, x, x)\n",
    "        x = attn_output.mean(dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "# class XceptionModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(XceptionModel, self).__init__()\n",
    "#         self.model = timm.create_model('xception', pretrained=True, num_classes=1000)\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "#         self.fc1 = nn.Linear(self.model.num_features, 512)\n",
    "#         self.bn = nn.BatchNorm1d(512)\n",
    "#         self.fc2 = nn.Linear(512, 2)\n",
    "#         self.model.fc = nn.Identity()  # Remove the original fully connected layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.model.forward_features(x)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.bn(x)\n",
    "#         x = nn.ReLU()(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "    \n",
    "# Instantiate models\n",
    "efficientnetb4 = EfficientNetB4()\n",
    "efficientnetattb4 = EfficientNetAutoAttB4()\n",
    "\n",
    "\n",
    "# Move models to device\n",
    "efficientnetb4 = efficientnetb4.to(device)\n",
    "efficientnetattb4 = efficientnetattb4.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xception Model\n",
    "class XceptionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XceptionModel, self).__init__()\n",
    "        self.model = timm.create_model('xception', pretrained=True, num_classes=1000)\n",
    "        self.model.fc = nn.Identity()  # Remove the original fully connected layer\n",
    "\n",
    "        # Calculate the number of features output by the forward_features method\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 224, 224)\n",
    "            dummy_output = self.model.forward_features(dummy_input)\n",
    "            num_features = dummy_output.view(dummy_output.size(0), -1).size(1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.bn = nn.BatchNorm1d(num_features)\n",
    "        self.fc2 = nn.Linear(num_features, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.forward_features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "#instantiate model\n",
    "xception = XceptionModel()\n",
    "\n",
    "#model to device\n",
    "xception = xception.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop with Mixed Precision and GradientScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_b4 = optim.Adam(efficientnetb4.parameters(), lr=0.0001)  #lr= 1e-4 \n",
    "optimizer_attb4 = optim.Adam(efficientnetattb4.parameters(), lr=0.0001)\n",
    "optimizer_xception = optim.Adam(xception.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "# Gradient Scaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return torch.sum(preds == labels.data).item()\n",
    "\n",
    "# Training function with gradient accumulation and mixed precision\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=8, accumulation_steps=2):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct_preds += calculate_accuracy(outputs, labels)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            # Memory cleanup\n",
    "            del inputs, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = correct_preds / total_samples * 100\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2060 SUPER'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetB4 Model Training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [35:25<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4, Loss: 0.1388, Accuracy: 89.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [35:19<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 0.0595, Accuracy: 95.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [30:52<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Loss: 0.0373, Accuracy: 97.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [31:00<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Loss: 0.0274, Accuracy: 98.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [33:07<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4, Loss: 0.0213, Accuracy: 98.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "#Train EfficientNetB4 Model\n",
    "print('EfficientNetB4 Model Training...\\n')\n",
    "efficientnetb4 = train_model(efficientnetb4, train_loader, criterion, optimizer_b4, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetAutoAttB4 Model training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [46:35<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4, Loss: 0.1250, Accuracy: 90.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [26:26<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 0.0577, Accuracy: 95.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [22:20<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Loss: 0.0378, Accuracy: 97.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [23:27<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Loss: 0.0266, Accuracy: 98.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [22:41<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4, Loss: 0.0210, Accuracy: 98.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#EfficientNetAutoAttB4 model\n",
    "print('EfficientNetAutoAttB4 Model training...\\n')\n",
    "efficientnetattb4 = train_model(efficientnetattb4, train_loader, criterion, optimizer_attb4, num_epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xception Model Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [13:58<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4, Loss: 0.1343, Accuracy: 90.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [13:54<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 0.0635, Accuracy: 95.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [13:53<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Loss: 0.0417, Accuracy: 96.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [13:53<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Loss: 0.0311, Accuracy: 97.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1678/1678 [14:27<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4, Loss: 0.0236, Accuracy: 98.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#XceptionNet Model Training\n",
    "print('Xception Model Training...')\n",
    "xception = train_model(xception, train_loader, criterion, optimizer_xception, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights are saved successfully.\n"
     ]
    }
   ],
   "source": [
    " # Save model weights\n",
    "# torch.save(efficientnetb4.state_dict(), 'efficientnetb4.pth')\n",
    "# torch.save(efficientnetattb4.state_dict(), 'efficientnetattb4.pth')\n",
    "# torch.save(xception.state_dict(), 'xception.pth')\n",
    "\n",
    "torch.save({\n",
    "    'efficientnetb4_state_dict': efficientnetb4.state_dict(),\n",
    "    'efficientnetattb4_state_dict': efficientnetattb4.state_dict(),\n",
    "    'xception_state_dict': xception.state_dict(),\n",
    "}, 'models.h5')\n",
    "\n",
    "print('Model weights are saved successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights are loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('models.h5')\n",
    "efficientnetb4.load_state_dict(checkpoint['efficientnetb4_state_dict'])\n",
    "efficientnetattb4.load_state_dict(checkpoint['efficientnetattb4_state_dict'])\n",
    "xception.load_state_dict(checkpoint['xception_state_dict'])\n",
    "\n",
    "print('Model weights are loaded successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: EfficientNetB4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 452/452 [09:50<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0293, Test Accuracy: 99.05%\n",
      "Total Classes Detected: 2\n",
      "Total Samples: 21685\n",
      "True Validation Samples (Correct Predictions): 21479\n",
      "Model Name: EfficientNetAutoAttB4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 452/452 [03:34<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0330, Test Accuracy: 98.76%\n",
      "Total Classes Detected: 2\n",
      "Total Samples: 21685\n",
      "True Validation Samples (Correct Predictions): 21417\n",
      "Model Name: XceptionNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 452/452 [03:12<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1019, Test Accuracy: 97.51%\n",
      "Total Classes Detected: 2\n",
      "Total Samples: 21685\n",
      "True Validation Samples (Correct Predictions): 21145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return torch.sum(preds == labels.data).item(), len(torch.unique(labels))\n",
    "\n",
    "# Function to load model weights and test the model\n",
    "def test_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "    total_classes = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#             correct_preds += calculate_accuracy(outputs, labels)\n",
    "#             total_samples += inputs.size(0)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct, unique_classes = calculate_accuracy(outputs, labels)\n",
    "            correct_preds += correct\n",
    "            total_samples += inputs.size(0)\n",
    "            total_classes.update(labels.cpu().numpy())\n",
    "\n",
    "            # Memory cleanup\n",
    "            del inputs, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    loss = running_loss / total_samples\n",
    "    accuracy = correct_preds / total_samples * 100\n",
    "    total_classes_detected = len(total_classes)\n",
    "    \n",
    "    print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Total Classes Detected: {total_classes_detected}')\n",
    "    print(f'Total Samples: {total_samples}')\n",
    "    print(f'True Validation Samples (Correct Predictions): {correct_preds}')\n",
    "\n",
    "# # Load model weights\n",
    "# efficientnetb4.load_state_dict(torch.load('efficientnetb4.pth'))\n",
    "# efficientnetattb4.load_state_dict(torch.load('efficientnetattb4.pth'))\n",
    "# xception.load_state_dict(torch.load('xception.pth'))\n",
    "\n",
    "# Test models\n",
    "print('Model Name: EfficientNetB4')\n",
    "test_model(efficientnetb4, test_loader, criterion)\n",
    "print('Model Name: EfficientNetAutoAttB4')\n",
    "test_model(efficientnetattb4, test_loader, criterion)\n",
    "print('Model Name: XceptionNet')\n",
    "test_model(xception, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Essembling and Inferencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 452/452 [05:37<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Test Loss: 0.0233, Ensemble Test Accuracy: 99.41%\n",
      "Total Classes Detected: 2\n",
      "Total Samples: 21685\n",
      "True Validation Samples (Correct Predictions): 21558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming models are already defined and loaded as efficientnetb4, efficientnetattb4, and xception\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def ensemble_models(models, inputs):\n",
    "    outputs = [model(inputs) for model in models]\n",
    "    avg_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "    return avg_output\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return torch.sum(preds == labels.data).item(), len(torch.unique(labels))\n",
    "\n",
    "def test_ensemble_model(models, dataloader, criterion):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        \n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "    total_classes = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = ensemble_models(models, inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct, unique_classes = calculate_accuracy(outputs, labels)\n",
    "            correct_preds += correct\n",
    "            total_samples += inputs.size(0)\n",
    "            total_classes.update(labels.cpu().numpy())\n",
    "\n",
    "            # Memory cleanup\n",
    "            del inputs, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    loss = running_loss / total_samples\n",
    "    accuracy = correct_preds / total_samples * 100\n",
    "    total_classes_detected = len(total_classes)\n",
    "    \n",
    "    print(f'Ensemble Test Loss: {loss:.4f}, Ensemble Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Total Classes Detected: {total_classes_detected}')\n",
    "    print(f'Total Samples: {total_samples}')\n",
    "    print(f'True Validation Samples (Correct Predictions): {correct_preds}')\n",
    "\n",
    "# Test the ensemble model\n",
    "models = [efficientnetb4, efficientnetattb4, xception]\n",
    "print('Ensemble Model')\n",
    "test_ensemble_model(models, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def ensemble_predict(models, input_image):\n",
    "    outputs = [model(input_image.to(device)) for model in models]\n",
    "    avg_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "    _, predicted = torch.max(avg_output, 1)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Yousuf Traders\\AppData\\Local\\Temp\\ipykernel_9096\\146582387.py\", line 73, in upload_video\n",
      "    result = process_video(video_path, models)\n",
      "                                       ^^^^^^\n",
      "NameError: name 'models' is not defined\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "# Preprocess frame\n",
    "def preprocess_frame(frame_path):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    image = Image.open(frame_path)\n",
    "    image = np.array(image)\n",
    "    image = transform(image=image)['image']\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "# Ensemble predict\n",
    "def ensemble_predict(models, input_image):\n",
    "    outputs = [model(input_image) for model in models]\n",
    "    avg_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "    prediction = torch.argmax(avg_output, dim=1)\n",
    "    return prediction\n",
    "\n",
    "# Extract frames\n",
    "def extract_frames(video_path, output_folder, interval=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % interval == 0:\n",
    "            frame_path = os.path.join(output_folder, f\"frame_{count}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    return count // interval  # Number of extracted frames\n",
    "\n",
    "# Process video\n",
    "def process_video(video_path, models):\n",
    "    frame_folder = 'temp_frames'\n",
    "    os.makedirs(frame_folder, exist_ok=True)\n",
    "    \n",
    "    # Extract frames from the video\n",
    "    num_frames = extract_frames(video_path, frame_folder)\n",
    "    \n",
    "    # Predict on each frame\n",
    "    real_count, fake_count = 0, 0\n",
    "    for i in range(num_frames):\n",
    "        frame_path = os.path.join(frame_folder, f'frame_{i * 30}.jpg')  # Considering the interval used in extract_frames\n",
    "        if os.path.exists(frame_path):\n",
    "            input_image = preprocess_frame(frame_path)\n",
    "            prediction = ensemble_predict(models, input_image)\n",
    "            if prediction.item() == 0:\n",
    "                real_count += 1\n",
    "            else:\n",
    "                fake_count += 1\n",
    "    \n",
    "    # Determine the overall result based on frame predictions\n",
    "    result = \"Real Video\" if real_count > fake_count else \"Fake Video\"\n",
    "    \n",
    "    # Clean up temporary frames\n",
    "    for frame_file in os.listdir(frame_folder):\n",
    "        os.remove(os.path.join(frame_folder, frame_file))\n",
    "    os.rmdir(frame_folder)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Tkinter interface\n",
    "def upload_video():\n",
    "    video_path = filedialog.askopenfilename(filetypes=[(\"Video files\", \".mp4;.avi;*.mov\")])\n",
    "    if video_path:\n",
    "        result = process_video(video_path, models)\n",
    "        messagebox.showinfo(\"Result\", f\"The uploaded video is a {result}\")\n",
    "\n",
    "# Create Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Deepfake Detection\")\n",
    "root.geometry(\"300x150\")\n",
    "\n",
    "# Upload button\n",
    "upload_button = tk.Button(root, text=\"Upload Video\", command=upload_video)\n",
    "upload_button.pack(expand=True)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3120670,
     "sourceId": 5380830,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
